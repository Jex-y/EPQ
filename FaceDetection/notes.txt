Working Title: Exploring the implementation and ethical consequenses of facial recgnition.
Aim is to first build a facial recognition system and then find ways to get around it. 
Official Yolov4 is  https://github.com/AlexeyAB/darknet but using https://github.com/hunglc007/tensorflow-yolov4-tflite as it uses tensorflow that I already know. 
It also supports tflite meaning that it can be used for edge computing such as phones and SOCs. 
Reference https://arxiv.org/abs/2004.10934
Workflow for final image should be preprocess -> face detection -> face recognition
Most of it should be done in notebooks as it can be easily presented.


[06/08/20]
Ok so not notebooks they are awful.
I am writing it as a library so that it can be reused by other people and myself in the future.
The model.py and layers.py use the same architecture from tensorflow-yolov4-tflite but written in a more tensorflow 2.0 like style.
I tried writing the dataset processing using tf.data and encoding it in .tfrecord files. This worked greate until the "if not exsists positive" part where the autograph function fails for some reason. 
I am not sure if it is a bug in tensorflow or I have misunderstood the framework.
Therefore I have two options: Submit a bug report and hope it gets fixed or someone points out my error, or I do the tfrecord parsing and data preprocessing in normal python and then take it to tf.data when I need to augment and batch the data.
I spend a while thinking about where the parsing and preprocess should be done. With tf.data I was going to make it part of the model and used a yolo superclass so that I can reuse the code.
Without tf.data I am not sure I think it should go in a seperate YOLODataset class which then has a to tf.data method.
Overall, good progress but its starting to slow. 

[08/08/20]
I am very behind schedule because I was initially unsure of if I was going to do the project. 
I need to do a literature review first. For this I am going to include the yolov3 and v4 papers as they are both useful resources. 
Also, I need to quicly read the book on AI ethics, the tensorflow 2.0 guide and finish super inteligence.
The model itself is currently a secondary prority as the paper itself must come first. 

Then I need to plan and draft the discussion. 

The good news is that I think that I have figured out the dataset flow. 

[09/08/20]
It is working I think. I have drafted the training loop, including tensorboard logging for the losses and learning rate. 
Overall, it is looking good. The training is working from the looks of things and it runs on a reasonable time scale, although it complains about memory. 
Still to do is to make tensorboard display the graph aswell so that I can generate nice graphics, find which layers need to be frozen for the second stage of training and actually test a model.
Then I can start work on the autoencoder part (still to be named).

[10/08/20]
So making tensorboard dipslay the model graph turns out to be very hard. 
Trying to implement it lead me down a path of sticking closer to the original keras.model class by overloading the make_train_function but I think that this would acctually be alot harder to do as the training loop is very custom.
The advantage of doing it that way would be that I could use built in callbacks for tensorboard and for lr scheduling but I wouldn't be able to use features like keras metrics as they just arn't applicable here. 
Overall, im sticking with the overloading fit function route as it is much simpler even if it is less pure.

[11/08/20]
It turns out that it is very hard to get tensorboard to dipslay the graph as well there isn't one as it is running in eager mode.
On the bright side the training seems to be working I just need to get the correct hyperparameters.
All the losses exept giou look ok so I increased the weighting of that to see if it helps.